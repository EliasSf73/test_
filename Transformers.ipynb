{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBLKNR9KfnyF4nYPiarnkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasSf73/test_/blob/master/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HirlUVIesR3W"
      },
      "outputs": [],
      "source": [
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Scaled-Dot_Product Attention\n",
        "\n",
        "The scaled_dot_product function is the heart of the attention mechanism in the MultiheadAttention module. It calculates the attention scores and applies them to the input values. Here's a step-by-step process:\n",
        "\n",
        "**Dot Product**: It computes the dot product between the query (q) and key (k) matrices, which indicates the degree to which each element of the query sequence should attend to each element of the key sequence.\n",
        "\n",
        "**Scaling**: The dot product is scaled down by the square root of the dimension of the key vectors. This helps stabilize gradients during training, as larger dimensions can cause the dot product to grow large in magnitude, leading to small gradients.\n",
        "\n",
        "**Masking (Optional)**: If a mask is provided, it is applied to the scaled dot products. This is often used to ignore padding tokens or future tokens in sequence-to-sequence tasks to prevent information leak.\n",
        "\n",
        "**Softmax Normalization**: A softmax function is applied to convert the scores into probabilities, which sum up to 1. This step ensures that the model's attention is distributed across the key elements.\n",
        "\n",
        "**Apply Attention to Values**: Finally, the function multiplies the attention probabilities by the value (v) matrix. This step aggregates the information from the values, weighted by the computed attention scores.\n",
        "\n",
        "The output of the scaled_dot_product function is a weighted sum of the values, giving us the final attention output that will be used by the rest of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Eddl8EuztAMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "  d_k=q.size()[-1]\n",
        "  #raw attention logits scores before scaling\n",
        "\n",
        "  attention_logits=torch.matmul(q,k.transpose(-2,-1))\n",
        "\n",
        "  #The attention scores are scaled by dividing them by the square root of the dimensionality of the keys (d_k).\n",
        "  #This scaling helps in stabilizing gradients during training, as it prevents the dot product from growing too large in magnitude.\n",
        "\n",
        "  attention_logits=attention_logits/math.sqrt(d_k)\n",
        "  # selectively ignore certain values to remove their positions from the attention calculation\n",
        "  if mask is not None:\n",
        "   attention_logits = attention_logits.masked_fill(mask == 0, -9e15)\n",
        "  # apply softmax along the last dimension to normalize attention scores; make them non-negative and sum=1. row scores--> actual attention weight\n",
        "  attention = F.softmax(attention_logits, dim=-1)\n",
        "  # aggregate information from different positions, weighted by the computed attention\n",
        "  values = torch.matmul(attention, v)\n",
        "\n",
        "  return values, attention\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLHGjB1utJki"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of what the function does"
      ],
      "metadata": {
        "id": "HEjQXXBtxYzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a seed value\n",
        "seed = 42\n",
        "# Set the seed for generating random numbers in PyTorch\n",
        "torch.manual_seed(seed)\n",
        "seq_len, d_k = 3, 2\n",
        "\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuSOcLkaw4Zd",
        "outputId": "5351bdf8-8586-45fb-d380-2e688c34ea42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863]])\n",
            "K\n",
            " tensor([[ 2.2082, -0.6380],\n",
            "        [ 0.4617,  0.2674],\n",
            "        [ 0.5349,  0.8094]])\n",
            "V\n",
            " tensor([[ 1.1103, -1.6898],\n",
            "        [-0.9890,  0.9580],\n",
            "        [ 1.3221,  0.8172]])\n",
            "Values\n",
            " tensor([[ 0.5698, -0.1520],\n",
            "        [ 0.5379, -0.0265],\n",
            "        [ 0.2246,  0.5556]])\n",
            "Attention\n",
            " tensor([[0.4028, 0.2886, 0.3086],\n",
            "        [0.3538, 0.3069, 0.3393],\n",
            "        [0.1303, 0.4630, 0.4067]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-head_attention**\n",
        "\n",
        "\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into â„Ž sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix.\n"
      ],
      "metadata": {
        "id": "FdSELdemyUSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to support different mask shapes.\n",
        "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
        "# If 2D: broadcasted over batch size and number of heads\n",
        "# If 3D: broadcasted over number of heads\n",
        "# If 4D: leave as is\n",
        "def expand_mask(mask):\n",
        "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
        "    if mask.ndim == 3:\n",
        "        mask = mask.unsqueeze(1)\n",
        "    while mask.ndim < 4:\n",
        "        mask = mask.unsqueeze(0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Qtw7WC4gynly"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Multihead Attention Mechanism\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The MultiheadAttention module is a core component of the Transformer architecture that allows the model to efficiently process sequences. It achieves this by focusing on different parts of the sequence simultaneously through multiple 'heads'. Each head learns different aspects of the data, providing a more nuanced understanding of the sequence.\n",
        "\n",
        "Here's a summary of its role:\n",
        "\n",
        "**Parallel Attention Heads**: Each head can attend to different parts of the sequence, capturing diverse relationships across the sequence. This parallel processing is key to the Transformer's ability to handle complex dependencies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Scalability**: Multiple attention heads make the model more flexible and scalable to large input sequences compared to single-head attention.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Learned Interactions**: The model learns different types of interactions between words (or subwords) in the sequence, which is critical for tasks such as language understanding and generation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**During the forward pass of the MultiheadAttention module**:\n",
        "\n",
        "Input sequences are linearly projected into queries (q), keys (k), and values (v) for each attention head.\n",
        "The scaled dot-product attention is computed for each set of queries, keys, and values.\n",
        "The attention outputs from each head are concatenated and linearly transformed into the final output.\n",
        "The MultiheadAttention module's ability to capture different representation subspaces at different positions makes it a powerful mechanism for sequence processing tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "UZDKHP6b694P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,input_dim,embed_dim,num_heads):\n",
        "    super().__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.embed_dim=embed_dim\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=embed_dim//num_heads\n",
        "    #Embedding size needs to be divisible by heads\n",
        "    assert (embed_dim==num_heads*self.head_dim)\n",
        "    #linear layers for transforming queries, keys, and values\n",
        "    self.W_q = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.W_k = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.W_v = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    #a fully connected output layer that combines the outputs from all heads.\n",
        "    self.fc_out = nn.Linear(num_heads * self.head_dim, embed_dim)\n",
        "\n",
        "    def forward(self,queries,keys,values,mask=None):\n",
        "      N = queries.shape[0]\n",
        "      query_len, key_len, value_len = queries.shape[1], keys.shape[1], values.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "      queries = queries.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "      keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "      values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "\n",
        "      queries = self.W_q(queries)\n",
        "      keys = self.W_k(keys)\n",
        "      values = self.W_v(values)\n",
        "\n",
        "      # Scaled Dot-Product Attention\n",
        "      energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "      if mask is not None:\n",
        "          energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "      attention = F.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "  # Apply attention weights to the values and then concatenate the heads' outputs.\n",
        "  # 'attention' (shape: [batch_size, num_heads, query_len, seq_len])\n",
        "  # 'values' (shape: [batch_size, seq_len, num_heads, head_dim])\n",
        "  # Output shape after einsum: [batch_size, query_len, num_heads, head_dim]\n",
        "  # Reshape to merge head dimension with the head_dim, forming the final output.\n",
        "      out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "      N, query_len, self.num_heads * self.head_dim\n",
        ")\n",
        "\n",
        "      #apply the fully connected output layer to the concatenated heads.\n",
        "\n",
        "      out = self.fc_out(out)\n",
        "      return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qLYHXdEI6-3t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENCODER-BLOCKS"
      ],
      "metadata": {
        "id": "tdS8y28IX-9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the EncoderBlock**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The EncoderBlock class is a fundamental building block of the Transformer\n",
        "\n",
        "\n",
        "\n",
        "Encoder. Each block performs two key operations:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Self-Attention**: This mechanism allows the encoder to consider other words in the input sequence when encoding a specific word, thereby capturing the context more effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Feed-Forward Network (FFN)**: After attention has been applied, the FFN further\n",
        "\n",
        "\n",
        "\n",
        "processes the attention output with two linear layers and a ReLU activation in between.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each EncoderBlock includes residual connections around these two operations, followed by layer normalization. The residual connections help in preserving the information from the initial input as it passes through multiple layers, preventing the vanishing gradient problem often seen in deep networks.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here's what happens during the forward pass of an EncoderBlock:\n",
        "\n",
        "Input x is first passed through the self-attention mechanism.\n",
        "The output of this attention, along with the original input, is normalized.\n",
        "The normalized output is then passed through the FFN.\n",
        "Finally, another layer of normalization is applied after adding the output of the FFN to the normalized attention output.\n",
        "This structure is repeated for each EncoderBlock within the TransformerEncoder, allowing for deep and complex transformation of the input sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "LrHrN8EDYCJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    # Initialize the EncoderBlock with attention and feedforward layers\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # Self-attention layer with multiple heads\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "        # Fully connected feed-forward network\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),  # First linear transformation\n",
        "            nn.Dropout(dropout),                    # Dropout for regularization\n",
        "            nn.ReLU(inplace=True),                  # ReLU activation\n",
        "            nn.Linear(dim_feedforward, input_dim)   # Second linear transformation\n",
        "        )\n",
        "        # Layer normalization modules\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        # Dropout module\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, x, mask=None):\n",
        "        # Apply self-attention to the input\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        # Add the attention output to the original input (residual connection) and normalize\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        # Apply the feed-forward network to the attention output\n",
        "        linear_out = self.linear_net(x)\n",
        "        #Applies dropout to the output of the MLP and adds it to the x before the MLP (another residual connection)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        #Normalizes the output from the previous step using the second layer normalization\n",
        "        x = self.norm2(x)\n",
        "        #Returns the final output of the encoder block.\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3eBQHP75YF5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder Overview\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The TransformerEncoder class is a key component of the Transformer architecture, responsible for processing the input sequence through multiple layers of attention and feed-forward networks. This class is designed to encapsulate the entire encoding mechanism of the Transformer model, which includes several layers of EncoderBlock modules stacked on top of each other.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here's a brief breakdown of the TransformerEncoder class functionality:\n",
        "\n",
        "**Initialization**: The TransformerEncoder initializes with a specified number of encoder layers (num_layers). Each layer is an instance of the EncoderBlock, which contains the self-attention mechanism and a position-wise feed-forward network.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**ModuleList**: It employs a nn.ModuleList to hold the EncoderBlock instances. This list ensures that each block's parameters are properly registered for training and that they are accessible during the forward pass.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Forward Pass**: During the forward pass, the input data x is sequentially processed by each EncoderBlock. If a mask is provided (e.g., to ignore padding in the input sequence), it is applied to each block's attention mechanism to prevent it from attending to padding tokens.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Output**: The output of the TransformerEncoder is a transformed sequence that has incorporated contextual information from the entire sequence. This output is then ready to be used by the next component in the Transformer model, typically a TransformerDecoder if the model is being used for tasks like machine translation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The encoder is a vital part of the Transformer model, enabling it to capture complex dependencies within the input data. The self-attention mechanism allows the model to focus on different parts of the input sequence, and the feed-forward networks further transform the data at each position."
      ],
      "metadata": {
        "id": "wV3goNPBaQgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            num_layers - Number of encoder blocks to stack\n",
        "            block_args - Arguments to be passed to each encoder block (e.g., input_dim, num_heads, dim_feedforward, dropout)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Stack multiple encoder blocks to form the encoder\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(**block_args) for _ in range(num_layers)\n",
        "        ])\n",
        "        # Store the number of layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Pass the input (and mask) through each encoder block in sequence\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6wkHKEiIaRsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}